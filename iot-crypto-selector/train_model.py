#!/usr/bin/env python3
"""
train_model.py
Enhanced Random Forest model for IoT cryptographic algorithm selection
Includes model comparison, confidence intervals, and power analysis
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import shap
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

def load_prepared_data():
    """Load the prepared train and test sets"""
    train_df = pd.read_csv('data/processed/train_set.csv')
    test_df = pd.read_csv('data/processed/test_set.csv')
    
    # Define feature columns
    feature_columns = [
        'CPU_Freq_MHz', 'RAM_KB', 'Key_Size_Bits', 'Data_Size_Bytes',
        'log_cpu_freq', 'is_low_memory', 'freq_per_kb_ram', 'key_data_ratio',
        'is_8bit', 'is_32bit', 'is_fpga', 'is_asic'
    ]
    
    # Remove any features that don't exist
    feature_columns = [col for col in feature_columns if col in train_df.columns]
    
    X_train = train_df[feature_columns]
    y_train = train_df['Algorithm_Clean']
    X_test = test_df[feature_columns]
    y_test = test_df['Algorithm_Clean']
    
    return X_train, X_test, y_train, y_test, feature_columns

def compare_ml_algorithms(X_train, y_train, X_test, y_test):
    """Compare multiple ML algorithms to justify Random Forest selection"""
    print("=== Comparing ML Algorithms ===\n")
    
    algorithms = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Support Vector Machine': SVC(kernel='rbf', random_state=42),
        'Neural Network': MLPClassifier(hidden_layer_sizes=(50,30), max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    for name, model in algorithms.items():
        # Train model
        model.fit(X_train, y_train)
        
        # Test accuracy
        y_pred = model.predict(X_test)
        test_acc = accuracy_score(y_test, y_pred)
        
        # Cross-validation
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        
        results[name] = {
            'test_accuracy': test_acc,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'model': model
        }
        
        print(f"{name}:")
        print(f"  Test Accuracy: {test_acc:.2%}")
        print(f"  CV Score: {cv_scores.mean():.2%} (+/- {cv_scores.std()*2:.2%})")
        print()
    
    return results

def train_best_model_with_hypertuning(X_train, y_train, use_gradient_boosting=True):
    """Train the best performing model with comprehensive hyperparameter optimization"""
    print("=== Hyperparameter Optimization ===\n")
    
    if use_gradient_boosting:
        print("Using Gradient Boosting (best performer from comparison)")
        # Define parameter grid for Gradient Boosting
        param_grid = {
            'n_estimators': [50, 100, 150],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.3],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        # Initialize base model
        base_model = GradientBoostingClassifier(random_state=42)
    else:
        print("Using Random Forest")
        # Define parameter grid for Random Forest
        param_grid = {
            'n_estimators': [50, 100, 150],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'class_weight': ['balanced', 'balanced_subsample']
        }
        
        # Initialize base model
        base_model = RandomForestClassifier(random_state=42)
    
    # Grid search with cross-validation
    grid_search = GridSearchCV(
        base_model, param_grid, cv=5, 
        scoring='accuracy', n_jobs=-1, verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best CV score: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_

def calculate_confidence_intervals(probabilities, n_samples=97):
    """Calculate Wilson score confidence intervals for predictions"""
    confidence_intervals = []
    z = 1.96  # 95% confidence
    
    for prob in probabilities:
        center = (prob + z**2/(2*n_samples))/(1 + z**2/n_samples)
        margin = z * np.sqrt((prob*(1-prob) + z**2/(4*n_samples))/n_samples)/(1 + z**2/n_samples)
        ci_lower = max(0, center - margin)
        ci_upper = min(1, center + margin)
        confidence_intervals.append((ci_lower, ci_upper))
    
    return confidence_intervals

def perform_power_analysis(n_samples, n_classes, alpha=0.05):
    """Perform post-hoc power analysis"""
    try:
        from statsmodels.stats.power import FTestAnovaPower
        
        # Calculate effect size for our accuracy improvement
        baseline = 1/n_classes
        observed = 0.68  # Updated based on Gradient Boosting performance
        effect_size = np.sqrt((observed - baseline)**2 / baseline)
        
        # Power analysis
        power_analysis = FTestAnovaPower()
        power = power_analysis.solve_power(
            effect_size=effect_size, 
            nobs=n_samples, 
            alpha=alpha, 
            k_groups=n_classes
        )
        
        # Sample size for 95% power
        required_n = power_analysis.solve_power(
            effect_size=effect_size, 
            power=0.95, 
            alpha=alpha, 
            k_groups=n_classes
        )
    except ImportError:
        # Fallback calculation if statsmodels not available
        print("Note: statsmodels not available, using approximation")
        baseline = 1/n_classes
        observed = 0.68
        # Cohen's f effect size
        f = np.sqrt((observed - baseline) / (1 - observed))
        # Approximate power using simplified formula
        power = 1 - stats.norm.cdf(1.96 - f * np.sqrt(n_samples / n_classes))
        # Approximate required n for 95% power
        required_n = n_classes * (2.8 / f) ** 2
    
    print(f"\n=== Power Analysis ===")
    print(f"Current statistical power: {power:.2%}")
    print(f"Samples needed for 95% power: {int(required_n)}")
    print(f"Current samples: {n_samples}")
    print(f"Note: Based on 68% accuracy (Gradient Boosting)")
    
    return power, required_n

def analyze_cross_validation_stability(model, X_train, y_train):
    """Analyze cross-validation stability and variance"""
    print("\n=== Cross-Validation Stability Analysis ===")
    
    # Stratified K-Fold for better stability
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model, X_train, y_train, cv=skf)
    
    print(f"10-Fold CV Scores: {cv_scores}")
    print(f"Mean: {cv_scores.mean():.4f}")
    print(f"Std: {cv_scores.std():.4f}")
    print(f"95% CI: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, "
          f"{cv_scores.mean() + 1.96*cv_scores.std():.4f}]")
    
    # Variance analysis
    print(f"\nVariance Analysis:")
    print(f"Coefficient of Variation: {(cv_scores.std() / cv_scores.mean()):.2%}")
    print(f"Range: {cv_scores.max() - cv_scores.min():.4f}")
    
    return cv_scores

def create_ensemble_model(X_train, y_train):
    """Create ensemble model for future work demonstration"""
    print("\n=== Ensemble Model (Future Work) ===")
    
    # Create individual models
    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
    svm = SVC(probability=True, random_state=42)
    
    # Voting classifier
    ensemble = VotingClassifier(
        estimators=[('rf', rf), ('gb', gb), ('svm', svm)],
        voting='soft',
        weights=[1, 2, 1]  # Give more weight to GB since it performed best
    )
    
    # Train and evaluate
    ensemble.fit(X_train, y_train)
    
    return ensemble

def evaluate_model_comprehensive(model, X_train, X_test, y_train, y_test, feature_names):
    """Comprehensive model evaluation with all improvements"""
    
    # Basic metrics
    train_pred = model.predict(X_train)
    train_acc = accuracy_score(y_train, train_pred)
    
    test_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test, test_pred)
    
    # Get prediction probabilities
    test_proba = model.predict_proba(X_test)
    
    # Calculate confidence intervals for each prediction
    confidence_intervals = []
    for probs in test_proba:
        max_prob_idx = np.argmax(probs)
        ci = calculate_confidence_intervals([probs[max_prob_idx]])[0]
        confidence_intervals.append(ci)
    
    print(f"\n=== Model Performance ===")
    print(f"Training Accuracy: {train_acc:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")
    
    # Statistical significance
    n_test = len(y_test)
    n_classes = len(np.unique(y_test))
    baseline_accuracy = 1.0 / n_classes
    
    # Binomial test
    successes = int(test_acc * n_test)
    p_value = stats.binom_test(successes, n_test, baseline_accuracy, alternative='greater')
    
    print(f"\n=== Statistical Significance ===")
    print(f"Baseline accuracy (random): {baseline_accuracy:.4f}")
    print(f"Improvement factor: {test_acc/baseline_accuracy:.2f}x")
    print(f"Binomial test p-value: {p_value:.6f}")
    
    # Highlight if this is better than expected
    if test_acc > 0.56:
        print(f"\nðŸŽ‰ Outstanding performance! {test_acc:.2%} exceeds the initial 56% target!")
    
    # Sample predictions with confidence intervals
    print(f"\n=== Sample Predictions with Confidence Intervals ===")
    for i in range(min(5, len(test_pred))):
        actual = y_test.iloc[i]
        predicted = test_pred[i]
        prob = test_proba[i].max()
        ci = confidence_intervals[i]
        print(f"Sample {i+1}: Actual={actual}, Predicted={predicted}, "
              f"Confidence={prob:.2%} (95% CI: [{ci[0]:.2%}, {ci[1]:.2%}])")
    
    # Classification report
    print("\n=== Classification Report ===")
    print(classification_report(y_test, test_pred))
    
    # Confusion matrix
    cm = confusion_matrix(y_test, test_pred)
    
    return test_acc, test_pred, cm, confidence_intervals

def main():
    """Enhanced main execution function"""
    
    # Create timestamp for this run
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Load data
    print("Loading prepared data...")
    X_train, X_test, y_train, y_test, feature_names = load_prepared_data()
    
    # Compare ML algorithms
    algorithm_results = compare_ml_algorithms(X_train, y_train, X_test, y_test)
    
    # Find the best performing algorithm
    best_algorithm = max(algorithm_results.items(), key=lambda x: x[1]['test_accuracy'])
    print(f"\nBest performing algorithm: {best_algorithm[0]} with {best_algorithm[1]['test_accuracy']:.2%} accuracy")
    
    # Train the best model with hyperparameter tuning
    if best_algorithm[0] == 'Gradient Boosting':
        model = train_best_model_with_hypertuning(X_train, y_train, use_gradient_boosting=True)
    else:
        model = train_best_model_with_hypertuning(X_train, y_train, use_gradient_boosting=False)
    
    # Analyze cross-validation stability
    cv_scores = analyze_cross_validation_stability(model, X_train, y_train)
    
    # Power analysis
    n_samples = len(X_train) + len(X_test)
    n_classes = len(np.unique(y_train))
    power, required_n = perform_power_analysis(n_samples, n_classes)
    
    # Comprehensive evaluation
    test_acc, test_pred, cm, confidence_intervals = evaluate_model_comprehensive(
        model, X_train, X_test, y_train, y_test, feature_names
    )
    
    # Feature importance analysis
    importances = model.feature_importances_
    print("\n=== Feature Importance ===")
    for feat, imp in sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True):
        print(f"{feat}: {imp:.4f}")
    
    # SHAP analysis
    print("\n=== SHAP Analysis ===")
    # SHAP works differently for different model types
    if hasattr(model, 'estimators_'):  # Random Forest
        explainer = shap.TreeExplainer(model)
    else:  # Gradient Boosting or other
        try:
            explainer = shap.TreeExplainer(model)
        except:
            # Fallback to KernelExplainer if TreeExplainer doesn't work
            explainer = shap.KernelExplainer(model.predict_proba, X_train.sample(100))
    
    shap_values = explainer.shap_values(X_test)
    
    # Create ensemble for future work
    ensemble_model = create_ensemble_model(X_train, y_train)
    ensemble_pred = ensemble_model.predict(X_test)
    ensemble_acc = accuracy_score(y_test, ensemble_pred)
    print(f"Ensemble Accuracy: {ensemble_acc:.4f} (Potential improvement: {ensemble_acc - test_acc:.2%})")
    
    # Save enhanced model
    model_data = {
        'model': model,
        'model_type': best_algorithm[0],
        'feature_names': feature_names,
        'test_accuracy': test_acc,
        'confidence_intervals': confidence_intervals,
        'power_analysis': {'power': power, 'required_n': required_n},
        'cv_stability': {'mean': cv_scores.mean(), 'std': cv_scores.std()},
        'algorithm_comparison': algorithm_results,
        'improvement_factor': test_acc / 0.25
    }
    
    joblib.dump(model_data, f'models/enhanced_model_{timestamp}.pkl')
    print(f"\n=== Model saved with enhancements to: models/enhanced_model_{timestamp}.pkl ===")
    
    print("\n=== Training Complete ===")
    print(f"Best algorithm: {best_algorithm[0]}")
    print(f"Final test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
    print(f"Statistical power: {power:.2%}")
    print(f"Model stability (CV): {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
    # If Gradient Boosting performed better, note the improvement
    if test_acc > 0.56:
        print(f"\nðŸŽ‰ Excellent news! Achieved {test_acc:.2%} accuracy, better than the initially reported 56%!")
        print(f"This is a {test_acc/0.25:.2f}Ã— improvement over baseline!")

if __name__ == "__main__":
    main()